{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TechBytes: Using Python with Teradata Vantage\n",
    "## Part 2: Data Exploration and Transformations - Building an ADS\n",
    "\n",
    "The contents of this file are Teradata Public Content and have been released to the Public Domain.\n",
    "Please see _license.txt_ file in the package for more information.\n",
    "\n",
    "Alexander Kolovos and Tim Miller - May 2021 - v.2.0 \\\n",
    "Copyright (c) 2021 by Teradata \\\n",
    "Licensed under BSD\n",
    "\n",
    "This TechByte demonstrates how to\n",
    "* explore your data in teradataml via functions for summary statistics, feature characteristics, data type reporting, and creating plots from table data.\n",
    "* use options to display the actual SQL query submitted by teradataml to the Database.\n",
    "* transform your data features by using teradataml DataFrame methods, Vantage Analytics Library (VAL) functions, and SQLAlchemy expressions.\n",
    "* persist a teradataml DataFrame as a table in the Database.\n",
    "\n",
    "Contributions by:\n",
    "- Alexander Kolovos, Sr Staff Software Architect, Teradata Product Engineering / Vantage Cloud and Applications.\n",
    "- Tim Miller, Principal Software Architect, Teradata Product Management / Advanced Analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Steps: Load libraries and create a Vantage connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T15:55:01.150305Z",
     "start_time": "2021-02-05T15:55:01.134716Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load teradataml and dependency packages.\n",
    "#\n",
    "import os\n",
    "import getpass as gp\n",
    "from teradataml import create_context, remove_context, get_context\n",
    "from teradataml import DataFrame, copy_to_sql, in_schema\n",
    "from teradataml.options.display import display\n",
    "\n",
    "# Import \"valib\" object from teradataml to execute Vantage Analytic Library functions.\n",
    "# Set 'configure.val_install_location' to the database name where Vantage analytic library functions are installed.\n",
    "from teradataml.analytics.valib import *\n",
    "from teradataml.options.configure import configure\n",
    "from teradataml.dataframe.sql_functions import case\n",
    "configure.val_install_location = \"TRNG_XSP\"\n",
    "\n",
    "from sqlalchemy.sql.expression import select, or_, extract, text, join, case as case_when\n",
    "from sqlalchemy import func\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a Teradata Vantage server to connect to. In the following statement, \n",
    "# replace the following argument values with strings as follows:\n",
    "# <HOST>   : Specify your target Vantage system hostname (or IP address).\n",
    "# <UID>    : Specify your Database username.\n",
    "# <PWD>    : Specify your password. You can also use encrypted passwords via\n",
    "#            the Stored Password Protection feature.\n",
    "#con = create_context(host = <HOST>, username = <UID>, password = <PWD>, \n",
    "#                     database = <DB_Name>, \"temp_database_name\" = <Temp_DB_Name>)\n",
    "#\n",
    "con = create_context(host = \"<Host_Name>\", username = \"<Username>\",\n",
    "                            password = gp.getpass(prompt='Password:'), \n",
    "                            logmech = \"LDAP\", database = \"TRNG_TECHBYTES\",\n",
    "                            temp_database_name = \"<Database_Name>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Database tables in teradataml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following data have been used for the present TechBytes series on Analytics with R and Python. They reside in the TRNG_XSP database on the Transcend server. It is a simplistic fictitious dataset of banking Customers (10K-ish rows), Accounts (20K-ish rows) and Transactions (1M-ish rows). The 3 subsets are related to each other in the following ways:\n",
    "\n",
    "![TWMDemoDataModel](Inputs/Plots/DemoData.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames for the Customer, Accounts and Transactions tables in the\n",
    "# Vantage Advanced SQL Engine. Using to_pandas() for a cleaner display format.\n",
    "# Note: to_pandas() converts a teradataml DataFrame to a pandas DataFrame. To\n",
    "#       this, it transfers all data of a table to the client; use with caution.\n",
    "# Note: in_schema() argument can read data from nondefault databases, as long\n",
    "#       as you have read permissions for the specified database.\n",
    "#\n",
    "tdCustomer = DataFrame(in_schema(\"TRNG_XSP\", \"Customer\")) \n",
    "tdCustomer.to_pandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdAccounts = DataFrame(in_schema(\"TRNG_XSP\", \"Accounts\"))\n",
    "tdAccounts.to_pandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdTransactions = DataFrame(in_schema(\"TRNG_XSP\", \"Transactions\"))\n",
    "tdTransactions.to_pandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Table info and summary statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inquire the data types of a table behind a teradataml DataFrame.\n",
    "#\n",
    "print(tdCustomer.tdtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Values function from VAL to inspect feature characteristics in the\n",
    "# tdCustomer teradataml dataset.\n",
    "#\n",
    "tdCustomer_values = valib.Values(data = tdCustomer, columns=[\"all\"])\n",
    "tdCustomer_values.result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Statistics function from VAL to compute summary statistics about the\n",
    "# tdCustomer teradataml dataset.\n",
    "tdCustomer_stats = valib.Statistics(data=tdCustomer, columns=\"allnumericanddate\")\n",
    "tdCustomer_stats.result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the teradataml option to print the SQL code of calls to Advanced SQL\n",
    "# or ML Engines analytic functions.\n",
    "#\n",
    "display.print_sqlmr_query = True\n",
    "\n",
    "# Use the show_query() method in analytic functions to display the SQL code\n",
    "# that teradataml pushes to the Database for execution. For example:\n",
    "#\n",
    "valib.Values(data = tdCustomer, columns=[\"all\"]).show_query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Plots from table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of income\n",
    "#\n",
    "tdCustomer_hist_pd = tdCustomer[tdCustomer.income != None].to_pandas()\n",
    "counts, bins = np.histogram(tdCustomer_hist_pd.income, bins = range(0, int(round(tdCustomer_hist_pd.income.max())), 10000))\n",
    "bins = 0.5 * (bins[:-1] + bins[1:])\n",
    "fig = px.bar(x = bins, y = counts)\n",
    "fig.update_layout(height = 400, title = \"Histogram of Income Distribution\")\n",
    "fig.update_xaxes(tickangle = 0, title = \"Income\")   \n",
    "fig.update_yaxes(title = \"Count\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot: Number of customers state-wise\n",
    "#\n",
    "tdCustomer_bar_pd = tdCustomer.groupby(\"state_code\").count().select(['state_code','count_cust_id']).sort(['state_code']).to_pandas()\n",
    "index = np.arange(tdCustomer_bar_pd.shape[0])\n",
    "fig = px.bar(x = index, y = tdCustomer_bar_pd['count_cust_id'])\n",
    "fig.update_layout(height = 400, title = \"Bar plot of Customers per State\")\n",
    "fig.update_xaxes(tickangle = 0, title = \"State ID\")   \n",
    "fig.update_yaxes(title = \"Count\")\n",
    "fig.show()\n",
    "\n",
    "#plt.bar(index, tdCustomer_bar_pd['count_cust_id'])\n",
    "#plt.xlabel(\"State ID\")\n",
    "#plt.ylabel(\"No of customers\")\n",
    "#plt.xticks(index, tdCustomer_bar_pd['state_code'], rotation=30)\n",
    "#plt.title(\"State-wise customers\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, grab the customer demographic variables and create indicator variables\n",
    "# for gender, marital_status and state_code (we consider both a classification\n",
    "# into the top 6 states and jointly the rest as \"OTHER\", as well as creating\n",
    "# indicator columns for each of the top 6 states).\n",
    "# \n",
    "# For this task, we illustrate data manipulation with the use of SQLAlchemy.\n",
    "# Initially we assemble the list of columns to be projected in SQL. We build it\n",
    "# by using the SQLAlchemy function case_when() and objects 'Column' and 'Case'.\n",
    "#\n",
    "cust_select_query_column_projection = [\n",
    "    tdCustomer.cust_id.expression,\n",
    "    tdCustomer.income.expression,\n",
    "    tdCustomer.age.expression,\n",
    "    tdCustomer.years_with_bank.expression.label(\"tot_cust_years\"),\n",
    "    tdCustomer.nbr_children.expression.label(\"tot_children\"),\n",
    "    case_when([(tdCustomer.gender.expression == \"M\", 1)], else_=0).expression.label(\"male_ind\"),\n",
    "    case_when([(tdCustomer.gender.expression == \"F\", 1)], else_=0).expression.label(\"female_ind\"),\n",
    "    case_when([(tdCustomer.marital_status.expression == \"1\", 1)], else_=0).expression.label(\"single_ind\"),\n",
    "    case_when([(tdCustomer.marital_status.expression == \"2\", 1)], else_=0).expression.label(\"married_ind\"),\n",
    "    case_when([(tdCustomer.marital_status.expression == \"3\", 1)], else_=0).expression.label(\"separated_ind\"),\n",
    "    case_when([(tdCustomer.state_code.expression == \"CA\", \"CA\"),\n",
    "               (tdCustomer.state_code.expression == \"NY\", \"NY\"),\n",
    "               (tdCustomer.state_code.expression == \"TX\", \"TX\"),\n",
    "               (tdCustomer.state_code.expression == \"IL\", \"IL\"),\n",
    "               (tdCustomer.state_code.expression == \"AZ\", \"AZ\"),\n",
    "               (tdCustomer.state_code.expression == \"OH\", \"OH\")\n",
    "              ], else_=\"OTHER\").expression.label(\"state_code\"),\n",
    "    case_when([(tdCustomer.state_code.expression == \"CA\", 1)], else_=0).expression.label(\"ca_resident_ind\"),\n",
    "    case_when([(tdCustomer.state_code.expression == \"NY\", 1)], else_=0).expression.label(\"ny_resident_ind\"),\n",
    "    case_when([(tdCustomer.state_code.expression == \"TX\", 1)], else_=0).expression.label(\"tx_resident_ind\"),\n",
    "    case_when([(tdCustomer.state_code.expression == \"IL\", 1)], else_=0).expression.label(\"il_resident_ind\"),\n",
    "    case_when([(tdCustomer.state_code.expression == \"AZ\", 1)], else_=0).expression.label(\"az_resident_ind\"),\n",
    "    case_when([(tdCustomer.state_code.expression == \"OH\", 1)], else_=0).expression.label(\"oh_resident_ind\")\n",
    "                                      ]\n",
    "#\n",
    "# Build and run the SQL from the list, then use it to create a DataFrame.\n",
    "#\n",
    "cust = DataFrame.from_query(str(select(cust_select_query_column_projection).compile(compile_kwargs={\"literal_binds\": True})))\n",
    "cust.to_pandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, get the accounts information. Create account indicators and calculate\n",
    "# account balances for each acct_type. Assume the median value for NaN balance\n",
    "# entries.\n",
    "#\n",
    "# For this task, we illustrate data manipulation with the use of VAL functions.\n",
    "#\n",
    "fn = FillNa(style = \"literal\", value = 0)\n",
    "\n",
    "account_type_t = OneHotEncoder(values = {\"CC\":\"cc_acct_ind\", \"CK\":\"ck_acct_ind\", \"SV\":\"sv_acct_ind\"}, \n",
    "                               columns = \"acct_type\", fillna = fn)\n",
    "fillna_t = FillNa(style = \"median\", columns = [\"cust_id\", \"starting_balance\", \"ending_balance\"])\n",
    "\n",
    "acctObj = valib.Transform(data = tdAccounts,\n",
    "                          one_hot_encode = [account_type_t],\n",
    "                          fillna = fillna_t,\n",
    "                          key_columns = \"cust_id\")\n",
    "\n",
    "acct_bal = acctObj.result.starting_balance + acctObj.result.ending_balance\n",
    "#\n",
    "# Build a DataFrame upon conclusion of the transformations.\n",
    "#\n",
    "acct = acctObj.result.assign(\n",
    "           cc_bal = case_when( [(acctObj.result.cc_acct_ind.expression == 1, acct_bal.expression)\n",
    "                               ], else_ = 0 )\n",
    "                            ).assign(\n",
    "           ck_bal = case_when( [(acctObj.result.ck_acct_ind.expression == 1, acct_bal.expression)\n",
    "                               ], else_ = 0 )\n",
    "                            ).assign(\n",
    "           sv_bal = case_when( [(acctObj.result.sv_acct_ind.expression == 1, acct_bal.expression)\n",
    "                               ], else_ = 0 )\n",
    "                            )\n",
    "acct.to_pandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, work on the transactions information. Add columns that indicate the \n",
    "# quarter a transaction was made.\n",
    "#\n",
    "acct_mon = extract('month', tdTransactions.tran_date.expression).expression\n",
    "\n",
    "trans = tdTransactions.assign(\n",
    "    q1_trans = case( [(acct_mon ==  \"1\", 1), (acct_mon ==  \"2\", 1), (acct_mon ==  \"3\", 1)], else_ = 0 ),\n",
    "    q2_trans = case( [(acct_mon ==  \"4\", 1), (acct_mon ==  \"5\", 1), (acct_mon ==  \"6\", 1)], else_ = 0 ),\n",
    "    q3_trans = case( [(acct_mon ==  \"7\", 1), (acct_mon ==  \"8\", 1), (acct_mon ==  \"9\", 1)], else_ = 0 ),\n",
    "    q4_trans = case( [(acct_mon == \"10\", 1), (acct_mon == \"11\", 1), (acct_mon == \"12\", 1)], else_ = 0 ),\n",
    "                             )\n",
    "trans.to_pandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we want to bring together the 3 transformed tables.\n",
    "#\n",
    "# Left-join the transformed Customer table to the transformed Account table.\n",
    "#\n",
    "cust_acct = cust.join(other = acct, how = \"left\", on = [\"cust_id\"],\n",
    "                             lsuffix = \"cust\", rsuffix = \"acct\")\n",
    "cust_acct.to_pandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, left-join the previous result to the transformed Transaction table.\n",
    "#\n",
    "acct_tran_amt = trans.principal_amt + trans.interest_amt\n",
    "\n",
    "cust_acct_tran = cust_acct.join(other = trans, how = \"left\", on = [\"acct_nbr\"], \n",
    "                                lsuffix = \"cu_ac\", rsuffix = \"trans\"\n",
    "                               ).assign(\n",
    "    cc_tran_amt = case_when( [(cust_acct.cc_acct_ind.expression == 1, acct_tran_amt.expression)\n",
    "                             ], else_=0 )\n",
    "                               ).assign(\n",
    "    ck_tran_amt = case_when( [(cust_acct.ck_acct_ind.expression == 1, acct_tran_amt.expression)\n",
    "                             ], else_=0 )\n",
    "                               ).assign(\n",
    "    sv_tran_amt = case_when( [(cust_acct.ck_acct_ind.expression == 1, acct_tran_amt.expression)\n",
    "                             ], else_=0 )\n",
    "                               )\n",
    "cust_acct_tran.to_pandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Building an Analytic Data Set (ADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, aggregate and roll up by 'cust_id' all variables to produce the\n",
    "# Analytic Data Set (ADS) we will be using for our analyses.\n",
    "#\n",
    "ADS_Py = cust_acct_tran.groupby(\"cust_cust_id\").agg(\n",
    "    {\n",
    "        \"income\"          : \"max\",\n",
    "        \"age\"             : \"max\",\n",
    "        \"tot_cust_years\"  : \"max\",\n",
    "        \"tot_children\"    : \"max\",\n",
    "        \"female_ind\"      : \"max\",\n",
    "        \"single_ind\"      : \"max\",\n",
    "        \"married_ind\"     : \"max\",\n",
    "        \"separated_ind\"   : \"max\",\n",
    "        \"state_code\"      : \"max\",\n",
    "        \"ca_resident_ind\" : \"max\",\n",
    "        \"ny_resident_ind\" : \"max\",\n",
    "        \"tx_resident_ind\" : \"max\",\n",
    "        \"il_resident_ind\" : \"max\",\n",
    "        \"az_resident_ind\" : \"max\",\n",
    "        \"oh_resident_ind\" : \"max\",\n",
    "        \"ck_acct_ind\"     : \"max\",\n",
    "        \"sv_acct_ind\"     : \"max\",\n",
    "        \"cc_acct_ind\"     : \"max\",\n",
    "        \"ck_bal\"          : \"mean\",\n",
    "        \"sv_bal\"          : \"mean\",\n",
    "        \"cc_bal\"          : \"mean\",\n",
    "        \"ck_tran_amt\"     : \"mean\",\n",
    "        \"sv_tran_amt\"     : \"mean\",\n",
    "        \"cc_tran_amt\"     : \"mean\",\n",
    "        \"q1_trans\"        : \"sum\",\n",
    "        \"q2_trans\"        : \"sum\",\n",
    "        \"q3_trans\"        : \"sum\",\n",
    "        \"q4_trans\"        : \"sum\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Rename columns\n",
    "\n",
    "columns = ['cust_id','income','age','tot_cust_years','tot_children',\n",
    "    'female_ind', 'single_ind', 'married_ind', 'separated_ind', 'state_code',\n",
    "    'ca_resident_ind', 'ny_resident_ind', 'tx_resident_ind','il_resident_ind',\n",
    "    'az_resident_ind', 'oh_resident_ind', 'ck_acct_ind', 'sv_acct_ind',\n",
    "    'cc_acct_ind', 'ck_avg_bal', 'sv_avg_bal', 'cc_avg_bal', 'ck_avg_tran_amt',\n",
    "    'sv_avg_tran_amt', 'cc_avg_tran_amt', 'q1_trans_cnt', 'q2_trans_cnt',\n",
    "    'q3_trans_cnt','q4_trans_cnt']\n",
    "\n",
    "ADS_Py = ADS_Py.assign(drop_columns = True,\n",
    "                       cust_id         = ADS_Py.cust_cust_id,\n",
    "                       income          = ADS_Py.max_income,\n",
    "                       age             = ADS_Py.max_age,\n",
    "                       tot_cust_years  = ADS_Py.max_tot_cust_years,\n",
    "                       tot_children    = ADS_Py.max_tot_children,\n",
    "                       female_ind      = ADS_Py.max_female_ind,\n",
    "                       single_ind      = ADS_Py.max_single_ind,\n",
    "                       married_ind     = ADS_Py.max_married_ind,\n",
    "                       separated_ind   = ADS_Py.max_separated_ind,\n",
    "                       state_code      = ADS_Py.max_state_code,\n",
    "                       ca_resident_ind = ADS_Py.max_ca_resident_ind,\n",
    "                       ny_resident_ind = ADS_Py.max_ny_resident_ind,\n",
    "                       tx_resident_ind = ADS_Py.max_tx_resident_ind,\n",
    "                       il_resident_ind = ADS_Py.max_il_resident_ind,\n",
    "                       az_resident_ind = ADS_Py.max_az_resident_ind,\n",
    "                       oh_resident_ind = ADS_Py.max_oh_resident_ind,\n",
    "                       ck_acct_ind     = ADS_Py.max_ck_acct_ind,\n",
    "                       sv_acct_ind     = ADS_Py.max_sv_acct_ind,\n",
    "                       cc_acct_ind     = ADS_Py.max_cc_acct_ind,\n",
    "                       ck_avg_bal      = ADS_Py.mean_ck_bal,\n",
    "                       sv_avg_bal      = ADS_Py.mean_sv_bal,\n",
    "                       cc_avg_bal      = ADS_Py.mean_cc_bal,\n",
    "                       ck_avg_tran_amt = ADS_Py.mean_ck_tran_amt,\n",
    "                       sv_avg_tran_amt = ADS_Py.mean_sv_tran_amt,\n",
    "                       cc_avg_tran_amt = ADS_Py.mean_cc_tran_amt,\n",
    "                       q1_trans_cnt    = ADS_Py.sum_q1_trans,\n",
    "                       q2_trans_cnt    = ADS_Py.sum_q2_trans,\n",
    "                       q3_trans_cnt    = ADS_Py.sum_q3_trans,\n",
    "                       q4_trans_cnt    = ADS_Py.sum_q4_trans\n",
    "                      ).select(columns)\n",
    "\n",
    "ADS_Py = ADS_Py.dropna()\n",
    "\n",
    "# teradataml DataFrame creates views at the backend which are temporary.\n",
    "# At the end of the context removal, all temporary table/views perish.\n",
    "# For this reason, persist the output of ADS_Py as a table in the SQLE.\n",
    "copy_to_sql(ADS_Py, table_name=\"ak_TBv2_ADS_Py\", if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a teradataml DataFrame and take a glimpse at it.\n",
    "#\n",
    "td_ADS_Py = DataFrame(\"ak_TBv2_ADS_Py\")\n",
    "td_ADS_Py.to_pandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the ADS into 2 samples, each with 60% and 40% of total rows.\n",
    "# Use the 60% sample to train, and the 40% sample to test/score.\n",
    "# Persist the samples as tables in the Database, and create DataFrames.\n",
    "#\n",
    "td_Train_Test_ADS = td_ADS_Py.sample(frac = [0.6, 0.4])\n",
    "\n",
    "Train_ADS = td_Train_Test_ADS[td_Train_Test_ADS.sampleid == \"1\"]\n",
    "copy_to_sql(Train_ADS, table_name=\"ak_TBv2_Train_ADS_Py\", if_exists=\"replace\")\n",
    "td_Train_ADS = DataFrame(\"ak_TBv2_Train_ADS_Py\")\n",
    "\n",
    "Test_ADS = td_Train_Test_ADS[td_Train_Test_ADS.sampleid == \"2\"]\n",
    "copy_to_sql(Test_ADS, table_name=\"ak_TBv2_Test_ADS_Py\", if_exists=\"replace\")\n",
    "td_Test_ADS = DataFrame(\"ak_TBv2_Test_ADS_Py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the context of present teradataml session and terminate the Python\n",
    "# session. It is recommended to call the remove_context() function for session\n",
    "# cleanup. Temporary objects are removed at the end of the session.\n",
    "#\n",
    "remove_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
