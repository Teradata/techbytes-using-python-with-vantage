{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TechBytes: Using Python with Teradata Vantage\n",
    "## Part 4: In-Database scripting with the SCRIPT Table Operator - Map functions\n",
    "\n",
    "The contents of this file are Teradata Public Content and have been released to the Public Domain.\n",
    "Please see _license.txt_ file in the package for more information.\n",
    "\n",
    "Alexander Kolovos and Tim Miller - May 2021 - v.2.0 \\\n",
    "Copyright (c) 2021 by Teradata \\\n",
    "Licensed under BSD\n",
    "\n",
    "This TechByte demonstrates a few different ways in which teradataml helps you use the SCRIPT Table Operator (STO) Database object for native execution of Python scripts inside the Database. Specifically, this TechByte shows how to\n",
    "* test a Python script on the client for correct in-Database execution with the teradataml STO Sandbox feature.\n",
    "* bring into the Database a Python model you have previously trained, and use it with a scoring script for in-Database scoring.\n",
    "* scale in-Database tasks of training and scoring with multiple models when using partitioned data.\n",
    "* use the teradataml DataFrame Map functions; namely, use the map_row() method for row-based operations and the map_partition() method for partition-based operations.\n",
    "\n",
    "_Note_: To perform in-nodes script execution, you need to coordinate with your Database Administrator (DBA), and ensure that (a) the STO Database object is activated in the Advanced SQL Engine, (b) the Teradata In-Nodes Python Interpreter and Add-ons packages are installed in the target server, and (c) your Database user account has the necessary STO permissions enabled by the DBA.\n",
    "\n",
    "Contributions by:\n",
    "- Alexander Kolovos, Sr Staff Software Architect, Teradata Product Engineering / Vantage Cloud and Applications.\n",
    "- Tim Miller, Principal Software Architect, Teradata Product Management / Advanced Analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Steps: Load libraries and create a Vantage connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-05T15:55:01.150305Z",
     "start_time": "2021-02-05T15:55:01.134716Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load teradataml and dependency packages.\n",
    "#\n",
    "import os\n",
    "import getpass as gp\n",
    "\n",
    "from teradataml import create_context, remove_context, get_context\n",
    "from teradataml import DataFrame, copy_to_sql, in_schema\n",
    "from teradataml.options.display import display\n",
    "\n",
    "from teradataml.table_operators.Script import Script\n",
    "from teradataml.table_operators.sandbox_container_util import *\n",
    "from teradatasqlalchemy import (VARCHAR, INTEGER, FLOAT, CLOB)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a Teradata Vantage server to connect to. In the following statement, \n",
    "# replace the following argument values with strings as follows:\n",
    "# <HOST>   : Specify your target Vantage system hostname (or IP address).\n",
    "# <UID>    : Specify your Database username.\n",
    "# <PWD>    : Specify your password. You can also use encrypted passwords via\n",
    "#            the Stored Password Protection feature.\n",
    "#con = create_context(host = <HOST>, username = <UID>, password = <PWD>, \n",
    "#                     database = <DB_Name>, \"temp_database_name\" = <Temp_DB_Name>)\n",
    "#\n",
    "con = create_context(host = \"<Host_Name>\", username = \"<Username>\",\n",
    "                            password = gp.getpass(prompt='Password:'), \n",
    "                            logmech = \"LDAP\", database = \"TRNG_TECHBYTES\",\n",
    "                            temp_database_name = \"<Database_Name>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a teradataml DataFrame from the ADS we need, and take a glimpse at it.\n",
    "#\n",
    "td_ADS_Py = DataFrame(\"ak_TBv2_ADS_Py\")\n",
    "td_ADS_Py.to_pandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the ADS into 2 samples, each with 60% and 40% of total rows.\n",
    "# Use the 60% sample to train, and the 40% sample to test/score.\n",
    "# Persist the samples as tables in the Database.\n",
    "#\n",
    "td_Train_Test_ADS = td_ADS_Py.sample(frac = [0.6, 0.4])\n",
    "\n",
    "Train_ADS = td_Train_Test_ADS[td_Train_Test_ADS.sampleid == \"1\"]\n",
    "copy_to_sql(Train_ADS, table_name=\"ak_TBv2_Train_ADS_Py\", if_exists=\"replace\")\n",
    "\n",
    "Test_ADS = td_Train_Test_ADS[td_Train_Test_ADS.sampleid == \"2\"]\n",
    "copy_to_sql(Test_ADS, table_name=\"ak_TBv2_Test_ADS_Py\", if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. BYOM: In-Database scoring with external Python model\n",
    "In this segment, we illustrate training a Random Forests classification model on the client, and saving it as a binary file. Eventually, we send the model together with a scoring script to the Database, and use them in the Database environment to scale a scoring task of a test dataset. In between, we demonstrate testing of the scoring script outside the Database in the teradataml STO Sandbox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Random Forests classification model training on the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the training subset of the ADS. First, read it from the table.\n",
    "# Then, convert to pandas DataFrame to enable subsequent modeling operations.\n",
    "#\n",
    "td_Train_ADS = DataFrame(\"ak_TBv2_Train_ADS_Py\")\n",
    "\n",
    "df_Train_ADS = td_Train_ADS.to_pandas()\n",
    "df_Train_ADS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the columns that the predictor accounts for.\n",
    "#\n",
    "predictor_columns = [\"income\", \"age\", \"tot_cust_years\", \"tot_children\",\n",
    "                     \"female_ind\", \"single_ind\", \"married_ind\", \"separated_ind\",\n",
    "                     \"ck_acct_ind\", \"sv_acct_ind\", \"ck_avg_bal\", \"sv_avg_bal\",\n",
    "                     \"ck_avg_tran_amt\", \"sv_avg_tran_amt\", \"q1_trans_cnt\",\n",
    "                     \"q2_trans_cnt\", \"q3_trans_cnt\", \"q4_trans_cnt\"]\n",
    "\n",
    "# Note: At time of creation of this TechByte, in-nodes Python has RF classifier\n",
    "#       from the scikit-learn add-on library v.0.22.2.post1. Keep an eye for  \n",
    "#       potential incompatibilites, if a package version on your client should\n",
    "#       differ from the in-nodes add-on version. \n",
    "#       In the present TechByte, the client carries scikit-learn add-on library\n",
    "#       v.0.23.2. No issues were observed when using a model built with this \n",
    "#       later version for in-nodes scoring.\n",
    "#       In case errors may be produced due to different scikit-learn versions \n",
    "#       on the client and in-nodes, you can try switching your client's add-on\n",
    "#       scikit-learn version to match in-nodes by using the explicit command:\n",
    "#       \"pip install scikit-learn==<version>\"\n",
    "\n",
    "# For the classifier, specify the following parameters:\n",
    "# ntree: n_estimators=500, mtry: max_features=5, nodesize: min_samples_leaf=1 (default; skipped)\n",
    "#\n",
    "classifier = RandomForestClassifier(n_estimators=500, max_features=5, random_state=0)\n",
    "X = df_Train_ADS[predictor_columns]\n",
    "y = df_Train_ADS[\"cc_acct_ind\"]\n",
    "\n",
    "# Train the Random Forest model to predict Credit Card account ownership based upon specified independent variables.\n",
    "#\n",
    "classifier = classifier.fit(X, y)\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model into file in present client folder where the notebook executes.\n",
    "# Note: In the following, we are using both pickle (to serialize) and base64\n",
    "#       (to encode) the model prior to saving it into a file. If model is only\n",
    "#       pickled, then unplickling in the Database might produce a pickle\n",
    "#       AttributeError that claims an \"X object has no attribute Y\". This is \n",
    "#       related to namespaces in client and target systems. See more info at: \n",
    "#       https://docs.python.org/3/library/pickle.html#pickling-class-instances\n",
    "#\n",
    "filePath = \"<your/path/to/folder/to/store/model/>\"\n",
    "modelFileName = \"RFmodel_py.out\"\n",
    "classifierPkl = pickle.dumps(classifier)\n",
    "classifierPklB64 = base64.b64encode(classifierPkl)\n",
    "with open(filePath + modelFileName, 'wb') as fOut:            # Write in binary format\n",
    "    fOut.write(classifierPklB64)\n",
    "\n",
    "print(\"Model saved in file '\" + filePath + modelFileName + \"'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Setting up the teradataml Script object and STO Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the path where we keep necessary files for this demo.\n",
    "#\n",
    "path_to_files = \"<your/path/to/folder/with/input/files/>\"\n",
    "#\n",
    "# Request to print the SQL submitted to the Advanced SQL Engine.\n",
    "#\n",
    "display.print_sqlmr_query = True\n",
    "\n",
    "# Set SQL SEARCHUIFDBPATH to database where script-related files are installed.\n",
    "#\n",
    "con.execute(\"SET SESSION SEARCHUIFDBPATH = TRNG_TECHBYTES;\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the teradataml DataFrame to use with the teradataml Script object.\n",
    "#\n",
    "td_Test_ADS = DataFrame(\"ak_TBv2_Test_ADS_Py\")\n",
    "\n",
    "# Recall that the testing subset table has an additional column with the sample\n",
    "# ID. We drop it because our script does not account for the extra column.\n",
    "td_Test_ADS = td_Test_ADS.drop(columns = \"sampleid\")\n",
    "\n",
    "# To test the script in the STO Sandbox, we would like to have a sample of\n",
    "# the test dataset handy for use in the STO Sandbox. To this end, we bring 500\n",
    "# rows of the test dataset from the Database, and save them into a csv file.\n",
    "# Note: Exclude the DataFrame index from the csv file; this prevents the index\n",
    "#       from being assumed to be an additional data column in the csv file. \n",
    "#\n",
    "df_Test_ADS_Sample = td_Test_ADS.to_pandas(num_rows = 500)\n",
    "df_Test_ADS_Sample.to_csv(path_to_files + \"stoSandboxTestData.csv\", index = False)\n",
    "df_Test_ADS_Sample.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a teradataml Script object. We will use this object with calls\n",
    "# to the STO Sandbox for testing and validation of the Python scoring script\n",
    "# before the script is sent to the Database.\n",
    "# Note: In present use case, the Python script will be importing a model file.\n",
    "#       Remember to adjust the relative model file location in the Python code.\n",
    "#       In the STO Sandbox, the model is expected to be found in the same,\n",
    "#       current Sandbox directory where the script is placed, too.\n",
    "#\n",
    "stoSB = Script(data = td_Test_ADS,\n",
    "               script_name = \"stoRFScoreSB.py\",\n",
    "               files_local_path = path_to_files, \n",
    "               script_command = \"python3 ./TRNG_TECHBYTES/stoRFScoreSB.py\",\n",
    "               delimiter = ',',\n",
    "               returns = { \"ID\": INTEGER(), \"Prob_0\": FLOAT(), \"Prob_1\": FLOAT(), \"Actual\": INTEGER() }\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the STO Sandbox with the Python STO Sandbox Docker image available\n",
    "# at downloads.teradata.com. Currently, when specifying the sandbox by image\n",
    "# location, then the \"sandbox_image_name\" must be specified, too, and must be\n",
    "# \"stosandbox:1.0\".\n",
    "#\n",
    "sb_path = \"<your/path/to/folder/where/sandbox/image/resides/>\"\n",
    "setup_sandbox_env(sandbox_image_location = sb_path + \"sto_sandbox_Python3.7.7_sles12sp3.0.5.4_docker_image.1.0.0.tar.gz\",\n",
    "                  sandbox_image_name = \"stosandbox:1.0\")\n",
    "\n",
    "configure.sandbox_container_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Script code testing in the STO Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testOut = stoSB.test_script(input_data_file = \"stoSandboxTestData.csv\",\n",
    "                            supporting_files = \"RFmodel_py.out\"\n",
    "                           )\n",
    "testOut.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the STO sandbox\n",
    "#\n",
    "cleanup_sandbox_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Script code execution in the Database with the STO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a different teradataml Script object. We will use this object with\n",
    "# calls to the STO for execution of the Python scoring script in-Database.\n",
    "# Note: In present use case, the Python script will be importing a model file.\n",
    "#       Remember to adjust the relative model file location in the Python code.\n",
    "#       In the Database, the model file is expected to be found inside the\n",
    "#       node directory that is named after the SEARCHUIFDBPATH.\n",
    "# In general, there is no need to define different teradataml Script objects\n",
    "# when the script code is identical for Sandbox and in-Database use.\n",
    "#\n",
    "sto = Script(data = td_Test_ADS,\n",
    "             script_name = \"stoRFScore.py\",\n",
    "             files_local_path = path_to_files, \n",
    "             script_command = \"python3 ./TRNG_TECHBYTES/stoRFScore.py\",\n",
    "             delimiter = ',',\n",
    "             returns = { \"ID\": INTEGER(), \"Prob_0\": FLOAT(), \"Prob_1\": FLOAT(), \"Actual\": INTEGER() }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If previous versions of files exist in the Database, remove them prior to\n",
    "# installing the current versions you wish to use.\n",
    "#\n",
    "sto.remove_file(file_identifier='RFmodel_py', force_remove=True)\n",
    "sto.remove_file(file_identifier='stoRFScore', force_remove=True)\n",
    "\n",
    "# Install the script and the accompanying model file in the target Advanced SQL\n",
    "# Engine Database. Remember to specify in your script code the correct path to\n",
    "# the model file in the Database: Your script will be looking for the model\n",
    "# file in a node directory named after the SEARCHUIFDBPATH.\n",
    "#\n",
    "sto.install_file(file_identifier='RFmodel_py', file_name='RFmodel_py.out', is_binary=True)\n",
    "sto.install_file(file_identifier='stoRFScore', file_name='stoRFScore.py', is_binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the script in-Database with the SCRIPT Table Operator.\n",
    "#\n",
    "sto.execute_script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Micromodeling: Scaled, In-Database training and scoring of multiple models\n",
    "When you need to train a different model for each value of a feature and then score corresponding data, Vantage and teradataml can help you scale the entire operation by training and scoring multiple models in parallel in the Advanced SQL Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the path where we keep necessary files for this demo.\n",
    "#\n",
    "path_to_files = \"<your/path/to/folder/with/input/files/>\"\n",
    "#\n",
    "# Request to print the SQL submitted to the Advanced SQL Engine.\n",
    "#\n",
    "display.print_sqlmr_query = True\n",
    "\n",
    "# Set SQL SEARCHUIFDBPATH to database where script-related files are installed.\n",
    "#\n",
    "con.execute(\"SET SESSION SEARCHUIFDBPATH = TRNG_TECHBYTES;\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Models training with the STO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Model training\n",
    "# \n",
    "# Start with the training subset of the ADS. First, read it from the table.\n",
    "# Then, convert to pandas DataFrame to enable subsequent modeling operations.\n",
    "#\n",
    "td_Train_ADS = DataFrame(\"ak_TBv2_Train_ADS_Py\")\n",
    "\n",
    "# Recall that the training subset table has an additional column of the sample\n",
    "# ID. We drop it because our script does not account for the extra column.\n",
    "td_Train_ADS = td_Train_ADS.drop(columns = \"sampleid\")\n",
    "td_Train_ADS.to_pandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the teradataml Script object for the training segment. This object\n",
    "# will be used to call the SCRIPT Table Operator in the Database. Specify that\n",
    "# we want to run the training script on data that should be partitioned by the\n",
    "# state code variable.\n",
    "# Note: In the present implementation, the output column names are used by the\n",
    "#       scoring script; handle naming carefully to maintain consistency with\n",
    "#       code and avoid errors during execution.\n",
    "#\n",
    "stoTr = Script(data = td_Train_ADS,\n",
    "               script_name = \"stoRFFitMM.py\",\n",
    "               files_local_path = path_to_files, \n",
    "               script_command = \"python3 ./TRNG_TECHBYTES/stoRFFitMM.py\",\n",
    "               data_partition_column = \"state_code\",\n",
    "               delimiter = ',',\n",
    "               returns = { \"State_Code\": VARCHAR(10), \"Model\": CLOB() }\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If previous versions of files exist in the Database, remove them prior to\n",
    "# installing the current versions you wish to use.\n",
    "#\n",
    "stoTr.remove_file(file_identifier='stoRFFitMM', force_remove=True)\n",
    "\n",
    "# Install the script and the accompanying model file in the target Advanced SQL\n",
    "# Engine Database.\n",
    "#\n",
    "stoTr.install_file(file_identifier='stoRFFitMM', file_name='stoRFFitMM.py', is_binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the script in-Database with the SCRIPT Table Operator.\n",
    "#\n",
    "trainOutObj = stoTr.execute_script()\n",
    "print(\"STO call complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trained models are now pointed to by the trainOutObj object.\n",
    "#\n",
    "# In the present illustration, we show how to use multipe trained models by\n",
    "# bringing them locally to the client and then uploading them as a file to\n",
    "# the Database. A different approach is to store the trained models directly\n",
    "# into a table in the Database. The latter approach is exhibited in the\n",
    "# section \"Using DataFrame.map_partition() Function for GLM Model Fitting and\n",
    "# Scoring Functions\" of the teradataml User Guide at docs.teradata.com.\n",
    "#\n",
    "# Save the trained models in a local csv file. The scoring script will need\n",
    "# this file to select and use the appropriate model with the corresponding \n",
    "# state code partition of test data on each Database AMP.\n",
    "#\n",
    "# To save the models into a file on the client, we first bring them locally\n",
    "# by converting the teradataml DataFrame to a pandas DataFrame.\n",
    "#\n",
    "multipleModels = trainOutObj.to_pandas()\n",
    "multipleModels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we simply export the pandas DataFrame to a csv file.\n",
    "# Note: Exclude the DataFrame index from the csv file; this prevents the index\n",
    "#       from being assumed by the Database to be an additional data column in\n",
    "#       the csv file. \n",
    "#\n",
    "multipleModels.to_csv(path_to_files + 'multipleModels_py.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Models scoring with the STO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Scoring with Models\n",
    "# \n",
    "# Specify the teradataml DataFrame to use with the teradataml Script object.\n",
    "#\n",
    "td_Test_ADS = DataFrame(\"ak_TBv2_Test_ADS_Py\")\n",
    "\n",
    "# Recall that the testing subset table has an additional column with the sample\n",
    "# ID. We drop it because our script does not account for the extra column.\n",
    "#\n",
    "td_Test_ADS = td_Test_ADS.drop(columns = \"sampleid\")\n",
    "td_Test_ADS.to_pandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the teradataml Script object. This object will be used with calls\n",
    "# to the SCRIPT Table Operator in the Database, as well as with STO script \n",
    "# testing in the STO Sandbox.\n",
    "# Note: In present use case, the Python script will be importing a model file.\n",
    "#       Remember to adjust the relative model file location in the Python code.\n",
    "#       In the Database, the model file is expected to be found inside the\n",
    "#       node directory that is named after the SEARCHUIFDBPATH.\n",
    "#\n",
    "stoSc = Script(data = td_Test_ADS,\n",
    "               script_name = \"stoRFScoreMM.py\",\n",
    "               files_local_path = path_to_files, \n",
    "               script_command = \"python3 ./TRNG_TECHBYTES/stoRFScoreMM.py\",\n",
    "               data_partition_column = \"state_code\",\n",
    "               delimiter = ',',\n",
    "               returns = { \"State_Code\": VARCHAR(10), \"Cust_ID\": INTEGER(), \n",
    "                           \"Prob_0\": FLOAT(), \"Prob_1\": FLOAT(), \"Actual\": INTEGER() }\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If previous versions of files exist in the Database, remove them prior to\n",
    "# installing the current versions you wish to use.\n",
    "#\n",
    "#stoSc.remove_file(file_identifier='multipleModels_py', force_remove=True)\n",
    "stoSc.remove_file(file_identifier='stoRFScoreMM', force_remove=True)\n",
    "\n",
    "# Install the script and the accompanying model file in the target Advanced SQL\n",
    "# Engine Database.Remember to specify in your script the correct path to the\n",
    "# model file in the Database: Your script needs to look for the script file in\n",
    "# a directory named after the SEARCHUIFDBPATH.\n",
    "#\n",
    "#stoSc.install_file(file_identifier='multipleModels_py', file_name='multipleModels_py.csv', is_binary=False)\n",
    "stoSc.install_file(file_identifier='stoRFScoreMM', file_name='stoRFScoreMM.py', is_binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the script in-Database with the SCRIPT Table Operator.\n",
    "#\n",
    "stoSc.execute_script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Map functions\n",
    "A different way to execute row-based or partition-based operation in the Advanced SQL Engine is via the teradataml DataFrame map_row() and map_partition() methods, respectively. In essence, these methods streamline the set-up and calls to the STO in the background, thus making interaction with the STO transparent to the Python user.\n",
    "\n",
    "**Caution:** For map_row() and map_partition() to work, teradataml requires the Python _dill_ add-on library version to be same on both the client and the target Advanced SQL Engine.\n",
    "\n",
    "In the following segment, we present brief examples of the map_row() and map_partition() methods. An additional micromodeling use case example with map_partition() can be found in the section \"Using DataFrame.map_partition() Function for GLM Model Fitting and Scoring Functions\" of the teradataml User Guide at docs.teradata.com. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from teradataml import load_example_data\n",
    "from collections import OrderedDict\n",
    "\n",
    "# This example uses the 'admissions_train' dataset, and calculates the average\n",
    "# 'gpa' per partition based on the value in 'admitted' column.\n",
    "#\n",
    "# Load the example data. Observe that the load_example_data() function creates\n",
    "# internal/temp tables, and for this reason it places them in the teradataml \n",
    "# context \"temp_database_name\" database, which presently is \"<Database_Name>\".\n",
    "# However, the teradataml DataFrame() function looks by default in the\n",
    "# teradataml context \"database\". Therefore, to create a teradataml DataFrame\n",
    "# in this case, the DataFrame() function must be pointed explicitly to look\n",
    "# into the \"<Database_Name>\" database, as shown in the following.\n",
    "#  \n",
    "load_example_data(\"dataframe\", \"admissions_train\")\n",
    "dfMap = DataFrame(in_schema('<Database_Name>', 'admissions_train'))\n",
    "dfMap.to_pandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A user defined function to increase the 'gpa' by a specified percentage.\n",
    "# Both the function input and output are pandas Series objects.\n",
    "#\n",
    "def increase_gpa(row, p = 20):\n",
    "    row['gpa'] = row['gpa'] + row['gpa'] * p/100\n",
    "    return row\n",
    "\n",
    "# Apply the user defined function to the teradataml DataFrame. The output of \n",
    "# the user defined function expects the same columns with the same types as\n",
    "# the input, hence the 'returns' argument of map_row() can be skipped.\n",
    "#\n",
    "increase_gpa_10 = dfMap.map_row(lambda row: increase_gpa(row, p = 10))\n",
    "#\n",
    "# Note: map_row() can ve also called with only the user-defined function as an\n",
    "#       argment; alternatively, it can be invoked with partial notation, too:\n",
    "#       increase_gpa_40 = df.map_row(lambda row: increase_gpa(row, p = 40))\n",
    "#       from functools import partial\n",
    "#       increase_gpa_50 = df.map_row(partial(increase_gpa, p = 50))\n",
    "#\n",
    "increase_gpa_10.to_pandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A user defined function to calculate the average 'gpa', by reading data at\n",
    "# once into a pandas DataFrame. The function accepts a TextFileReader object\n",
    "# for data iteration in chunks. The function returns a pandas Series.\n",
    "#\n",
    "def grouped_gpa_avg(rows):\n",
    "    pdf = rows.read()\n",
    "    if pdf.shape[0] > 0:\n",
    "        return pdf[['admitted', 'gpa']].mean()\n",
    "\n",
    "# Apply the user defined function to the DataFrame.\n",
    "#\n",
    "avg_gpa_pdf = dfMap.map_partition(\n",
    "                        grouped_gpa_avg,\n",
    "                        returns = OrderedDict([('admitted', INTEGER()), ('avg_gpa', FLOAT())]),\n",
    "                        data_partition_column = 'admitted'\n",
    "                                 )\n",
    "avg_gpa_pdf.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the context of present teradataml session and terminate the Python\n",
    "# session. It is recommended to call the remove_context() function for session\n",
    "# cleanup. Temporary objects are removed at the end of the session.\n",
    "#\n",
    "remove_context()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
